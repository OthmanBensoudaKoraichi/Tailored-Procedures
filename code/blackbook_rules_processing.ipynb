{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "317f594c",
   "metadata": {},
   "source": [
    "# Extract all orders from markdown blackbooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9e6a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Blackbook Extraction ‚Äî ORDER LEVEL (All Files)\n",
    "Chunking ‚âà 5000 chars, extends to next Effective boundary\n",
    "Issued year priority:\n",
    "Filed > Dated > Approved > Effective\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "LLM_MODEL = \"gpt-5-mini\"\n",
    "\n",
    "INPUT_DIR = Path(\n",
    "    \"/Users/othmanbensouda/Desktop/Orion/jobtalk_paper/files/blackbooks/md_format_clean\"\n",
    ")\n",
    "\n",
    "OUTPUT_FILE = Path(\n",
    "    \"/Users/othmanbensouda/Desktop/Orion/jobtalk_paper/files/order_extraction/extracted_orders_all_files.xlsx\"\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SCHEMA\n",
    "# ============================================================\n",
    "\n",
    "class OrderEntry(BaseModel):\n",
    "    order_title: str\n",
    "    filed_date: Optional[str] = None\n",
    "    dated_date: Optional[str] = None\n",
    "    approved_date: Optional[str] = None\n",
    "    effective_date: Optional[str] = None\n",
    "\n",
    "\n",
    "class OrdersList(BaseModel):\n",
    "    orders: List[OrderEntry]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CHUNKING (~5000 chars, extend to next Effective boundary)\n",
    "# ============================================================\n",
    "\n",
    "def chunk_page_extend_to_effective(text: str):\n",
    "\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    effective_pattern = re.compile(\n",
    "        r\"Effective\\s*:?\\s*\"\n",
    "        r\"(?:\\d{1,2}/\\d{1,2}/\\d{4}\"\n",
    "        r\"|[A-Za-z]+\\s+\\d{1,2},?\\s+\\d{4})\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    TARGET_SIZE = 5000\n",
    "    MAX_EXTENSION = 15000  # safety cap\n",
    "\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    n = len(text)\n",
    "\n",
    "    while i < n:\n",
    "\n",
    "        tentative_end = min(i + TARGET_SIZE, n)\n",
    "\n",
    "        match = effective_pattern.search(text, tentative_end)\n",
    "\n",
    "        if match:\n",
    "            end = match.end()\n",
    "\n",
    "            # prevent runaway large chunk\n",
    "            if end - i > MAX_EXTENSION:\n",
    "                end = tentative_end\n",
    "        else:\n",
    "            end = n\n",
    "\n",
    "        chunk = text[i:end].strip()\n",
    "\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        i = end\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PROMPT\n",
    "# ============================================================\n",
    "\n",
    "def build_prompt(chunk: str) -> str:\n",
    "    return f\"\"\"\n",
    "Extract complete court orders.\n",
    "\n",
    "An order usually contains a heading/title, and at least one date. You might retrieve an order that has no date associated.\n",
    "\n",
    "Note that an order is literally any paragraph that has a date after, such as 'AMENDING RULES 47.1, 48, AND 79, RULES OF PROCEDURE FOR THE JUVENILE COURT, ON A PERMANENT BASIS\n",
    "Filed: 12/12/2019\n",
    "Effective 12/12/2019'\n",
    "\n",
    "Additionally extract if present:\n",
    "‚Ä¢ Filed: date\n",
    "‚Ä¢ Dated: date\n",
    "‚Ä¢ Approved: date\n",
    "\n",
    "Return structured output only.\n",
    "\n",
    "TEXT:\n",
    "{chunk}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ISSUED YEAR LOGIC (DETERMINISTIC)\n",
    "# ============================================================\n",
    "\n",
    "def extract_year(date_str):\n",
    "    if not date_str:\n",
    "        return None\n",
    "    match = re.search(r\"\\d{4}\", str(date_str))\n",
    "    return int(match.group()) if match else None\n",
    "\n",
    "\n",
    "def compute_issued_year(row):\n",
    "    for field in [\"filed_date\", \"dated_date\", \"approved_date\", \"effective_date\"]:\n",
    "        year = extract_year(row.get(field))\n",
    "        if year:\n",
    "            return year\n",
    "    return None\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "\n",
    "    print(\"üöÄ Starting extraction (all files)\")\n",
    "\n",
    "    if not INPUT_DIR.exists():\n",
    "        print(\"‚ùå Input directory not found:\", INPUT_DIR)\n",
    "        return\n",
    "\n",
    "    md_files = sorted(INPUT_DIR.glob(\"*.md\"))\n",
    "\n",
    "    if not md_files:\n",
    "        print(\"‚ùå No markdown files found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(md_files)} markdown files.\")\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        model=LLM_MODEL,\n",
    "    )\n",
    "\n",
    "    structured_llm = llm.with_structured_output(OrdersList)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # PRECOMPUTE TOTAL CHUNKS\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    file_chunks_map = {}\n",
    "    total_chunks = 0\n",
    "\n",
    "    for f in md_files:\n",
    "        text = f.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        text = re.sub(r\"<!--.*?-->\", \"\", text, flags=re.S)\n",
    "        chunks = chunk_page_extend_to_effective(text)\n",
    "\n",
    "        file_chunks_map[f] = chunks\n",
    "        total_chunks += len(chunks)\n",
    "\n",
    "    print(f\"Total chunks to process: {total_chunks}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # PROCESS CHUNKS WITH GLOBAL tqdm\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    with tqdm(total=total_chunks, desc=\"Chunks\") as pbar:\n",
    "\n",
    "        for f, chunks in file_chunks_map.items():\n",
    "\n",
    "            if not chunks:\n",
    "                tqdm.write(f\"‚ö†Ô∏è No chunks in {f.name}\")\n",
    "                continue\n",
    "\n",
    "            for chunk in chunks:\n",
    "\n",
    "                try:\n",
    "                    result = structured_llm.invoke(build_prompt(chunk))\n",
    "                    rows = [o.model_dump() for o in result.orders]\n",
    "\n",
    "                    for r in rows:\n",
    "                        r[\"source_file\"] = f.name\n",
    "\n",
    "                    all_rows.extend(rows)\n",
    "\n",
    "                except Exception as e:\n",
    "                    tqdm.write(f\"‚ùå LLM ERROR in {f.name}: {e}\")\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "            # --------------------------------------------------\n",
    "            # SAVE AFTER EACH FILE\n",
    "            # --------------------------------------------------\n",
    "\n",
    "            if all_rows:\n",
    "\n",
    "                df = pd.DataFrame(all_rows)\n",
    "\n",
    "                df[\"issued_year\"] = df.apply(compute_issued_year, axis=1)\n",
    "\n",
    "                df[\"order_title_norm\"] = (\n",
    "                    df[\"order_title\"]\n",
    "                    .str.lower()\n",
    "                    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "                    .str.strip()\n",
    "                )\n",
    "\n",
    "                df = df.drop_duplicates(\n",
    "                    subset=[\"order_title_norm\", \"effective_date\"],\n",
    "                    keep=\"first\",\n",
    "                ).drop(columns=[\"order_title_norm\"])\n",
    "\n",
    "                df.to_excel(OUTPUT_FILE, index=False)\n",
    "\n",
    "                tqdm.write(\n",
    "                    f\"üíæ Saved after {f.name} \"\n",
    "                    f\"({len(df)} total orders so far)\"\n",
    "                )\n",
    "\n",
    "    print(f\"\\n‚úÖ Finished processing all chunks.\")\n",
    "    print(f\"Final output saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d4624",
   "metadata": {},
   "source": [
    "# Extract bodies of rules from orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4f0dae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'genai' from 'google' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconcurrent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor, as_completed\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m genai\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HttpOptions\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# CONFIG\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'genai' from 'google' (unknown location)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Blackbook Extraction ‚Äî BODY OF RULE LEVEL\n",
    "Splits previously extracted orders into one row per body of rules\n",
    "- Uses project-relative paths\n",
    "- Handles 1, 2, 3, 4+ rule bodies\n",
    "- Reattaches bracket codes and dates to every row\n",
    "- Deterministic fallback if LLM fails\n",
    "- MULTITHREADED for faster processing\n",
    "- EXTRACTS body of rules name to new column\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "from tqdm import tqdm\n",
    "from langchain_openai import ChatOpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG (PROJECT-RELATIVE)\n",
    "# ============================================================\n",
    "LLM_MODEL = \"gpt-5\"\n",
    "MAX_WORKERS = 500\n",
    "\n",
    "BASE_DIR = Path.cwd().parent  \n",
    "\n",
    "INPUT_FILE = BASE_DIR / \"files\" / \"order_extraction\" / \"extracted_orders_all_files.xlsx\"\n",
    "OUTPUT_FILE = BASE_DIR / \"files\" / \"order_extraction\" / \"extracted_rule_bodies.xlsx\"\n",
    "\n",
    "# ============================================================\n",
    "# STRUCTURED OUTPUT SCHEMA\n",
    "# ============================================================\n",
    "class SplitBody(BaseModel):\n",
    "    body_text: str\n",
    "    body_name: str  # e.g., \"Arizona Rules of Civil Procedure\"\n",
    "\n",
    "class SplitBodiesList(BaseModel):\n",
    "    bodies: List[SplitBody]\n",
    "\n",
    "# ============================================================\n",
    "# PROMPT\n",
    "# ============================================================\n",
    "def build_split_prompt(order_title: str) -> str:\n",
    "    return f\"\"\"\n",
    "Split this court order into separate rows, ONE ROW PER DISTINCT BODY OF RULES.\n",
    "\n",
    "For EACH body, you must provide:\n",
    "1. body_text: The full text of that row (with order number, rules, bracket code, dates)\n",
    "2. body_name: JUST the name of the rule system (e.g., \"Arizona Rules of Civil Procedure\")\n",
    "\n",
    "CRITICAL RULE for body_text:\n",
    "Each row must include:\n",
    "- The order number (e.g., \"(13)\")\n",
    "- The specific rules for that body\n",
    "- The bracket code (e.g., [R-18-0008])\n",
    "- ALL dates (Filed/Effective/Dated/Approved)\n",
    "\n",
    "CRITICAL RULE for body_name:\n",
    "Extract ONLY the rule system name, such as:\n",
    "- \"Arizona Rules of Civil Procedure\"\n",
    "- \"Arizona Rules of Criminal Procedure\"\n",
    "- \"Arizona Rules of Evidence\"\n",
    "- \"Rules of Evidence\"\n",
    "- \"Arizona Rules of Procedure for the Juvenile Court\"\n",
    "- \"Arizona Rule of Procedure for Eviction Actions\"\n",
    "- \"Rules of the Supreme Court\"\n",
    "\n",
    "SPLITTING TRIGGERS:\n",
    "- Semicolons separating rule systems\n",
    "- \"AND [RULE SYSTEM NAME]\" introducing a new system\n",
    "- Any new distinct named rule system\n",
    "\n",
    "DO NOT SPLIT when \"and\" just connects rule numbers in the same system.\n",
    "\n",
    "EXAMPLE (REQUIRED FORMAT):\n",
    "Input: (13) ORDER AMENDING RULES OF EVIDENCE 1001, 1002, 1004, 1006, 1007, 1008; ARIZONA RULES OF CRIMINAL PROCEDURE 15.1, 15.2, 15.3; ARIZONA RULES OF PROCEDURE FOR THE JUVENILE COURT 16, 44, 73; and ARIZONA RULE OF PROCEDURE FOR EVICTION ACTIONS 10 [R-18-0008] Filed: 08/28/2018 Effective: 01/01/2019\n",
    "\n",
    "Expected output (4 separate bodies):\n",
    "Body 1: \n",
    "  body_text: \"(13) ORDER AMENDING RULES OF EVIDENCE 1001, 1002, 1004, 1006, 1007, 1008; [R-18-0008] Filed: 08/28/2018 Effective: 01/01/2019\"\n",
    "  body_name: \"Rules of Evidence\"\n",
    "\n",
    "Body 2:\n",
    "  body_text: \"ARIZONA RULES OF CRIMINAL PROCEDURE 15.1, 15.2, 15.3 [R-18-0008] Filed: 08/28/2018 Effective: 01/01/2019\"\n",
    "  body_name: \"Arizona Rules of Criminal Procedure\"\n",
    "\n",
    "Body 3:\n",
    "  body_text: \"ARIZONA RULES OF PROCEDURE FOR THE JUVENILE COURT 16, 44, 73 [R-18-0008] Filed: 08/28/2018 Effective: 01/01/2019\"\n",
    "  body_name: \"Arizona Rules of Procedure for the Juvenile Court\"\n",
    "\n",
    "Body 4:\n",
    "  body_text: \"ARIZONA RULE OF PROCEDURE FOR EVICTION ACTIONS 10 [R-18-0008] Filed: 08/28/2018 Effective: 01/01/2019\"\n",
    "  body_name: \"Arizona Rule of Procedure for Eviction Actions\"\n",
    "\n",
    "NOW PROCESS:\n",
    "{order_title}\n",
    "\"\"\".strip()\n",
    "\n",
    "# ============================================================\n",
    "# PROCESSING FUNCTION\n",
    "# ============================================================\n",
    "def process_row(row_data):\n",
    "    \"\"\"Process a single row - this will be called in parallel\"\"\"\n",
    "    idx, row = row_data\n",
    "    \n",
    "    # Each thread gets its own LLM instance\n",
    "    llm = ChatOpenAI(model=LLM_MODEL)\n",
    "    structured_llm = llm.with_structured_output(SplitBodiesList)\n",
    "    \n",
    "    try:\n",
    "        result = structured_llm.invoke(\n",
    "            build_split_prompt(row[\"order_title\"])\n",
    "        )\n",
    "        \n",
    "        bodies = [(b.body_text.strip(), b.body_name.strip()) for b in result.bodies]\n",
    "        \n",
    "        if not bodies:\n",
    "            bodies = [(row[\"order_title\"], \"Unknown\")]\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Fallback to original title on error\n",
    "        bodies = [(row[\"order_title\"], \"Unknown\")]\n",
    "    \n",
    "    # Create one row per body\n",
    "    result_rows = []\n",
    "    for body_text, body_name in bodies:\n",
    "        new_row = row.to_dict()  # FIXED: Convert to dict\n",
    "        new_row[\"order_title\"] = body_text\n",
    "        new_row[\"body_of_rules\"] = body_name\n",
    "        result_rows.append(new_row)\n",
    "    \n",
    "    return result_rows\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "def main():\n",
    "    print(\"üöÄ Starting rule-body extraction (MULTITHREADED)\")\n",
    "    \n",
    "    if not INPUT_FILE.exists():\n",
    "        print(\"‚ùå Input file not found:\", INPUT_FILE)\n",
    "        return\n",
    "    \n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "    \n",
    "    print(f\"Processing {len(df)} orders with {MAX_WORKERS} workers...\")\n",
    "    \n",
    "    all_rows = []\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit all tasks\n",
    "        futures = {\n",
    "            executor.submit(process_row, (idx, row)): idx \n",
    "            for idx, row in df.iterrows()\n",
    "        }\n",
    "        \n",
    "        # Process results as they complete\n",
    "        with tqdm(total=len(futures), desc=\"Orders\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    result_rows = future.result()\n",
    "                    all_rows.extend(result_rows)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error processing row: {e}\")\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    final_df = pd.DataFrame(all_rows)\n",
    "    \n",
    "    # Reorder columns to put body_of_rules first\n",
    "    if 'body_of_rules' in final_df.columns:\n",
    "        cols = ['body_of_rules'] + [col for col in final_df.columns if col != 'body_of_rules']\n",
    "        final_df = final_df[cols]\n",
    "    \n",
    "    print(f\"\\nüìä Columns: {list(final_df.columns)}\")\n",
    "    if 'body_of_rules' in final_df.columns:\n",
    "        print(f\"Sample body_of_rules values:\\n{final_df['body_of_rules'].value_counts().head(10)}\")\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # DEDUPLICATION\n",
    "    # --------------------------------------------------------\n",
    "    final_df[\"rule_body_norm\"] = (\n",
    "        final_df[\"order_title\"]\n",
    "        .str.lower()\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "    \n",
    "    initial_count = len(final_df)\n",
    "    \n",
    "    final_df = final_df.drop_duplicates(\n",
    "        subset=[\"rule_body_norm\", \"effective_date\"],\n",
    "        keep=\"first\"\n",
    "    ).drop(columns=[\"rule_body_norm\"])\n",
    "    \n",
    "    duplicates_removed = initial_count - len(final_df)\n",
    "    \n",
    "    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "    final_df.to_excel(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(\"\\n‚úÖ Finished.\")\n",
    "    print(f\"Input rows: {len(df)}\")\n",
    "    print(f\"Output rows: {len(final_df)}\")\n",
    "    print(f\"Duplicates removed: {duplicates_removed}\")\n",
    "    print(\"Saved to:\", OUTPUT_FILE)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777fe337",
   "metadata": {},
   "source": [
    "# Create an \"Issued_Date\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create issued_date column from filed_date, dated_date, approved_date, effective_date\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "BASE_DIR = Path.cwd().parent  \n",
    "\n",
    "INPUT_FILE = BASE_DIR / \"files\" / \"order_extraction\" / \"extracted_rule_bodies.xlsx\"\n",
    "OUTPUT_FILE = BASE_DIR / \"files\" / \"order_extraction\" / \"extracted_rule_bodies.xlsx\"\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "def main():\n",
    "    print(\"üöÄ Creating issued_date column\")\n",
    "    \n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "    \n",
    "    # Create issued_date: use first non-null value\n",
    "    df['issued_date'] = (df['filed_date']\n",
    "                         .fillna(df['dated_date'])\n",
    "                         .fillna(df['approved_date'])\n",
    "                         .fillna(df['effective_date']))\n",
    "    \n",
    "    df.to_excel(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(\"‚úÖ Done\")\n",
    "    print(f\"Rows: {len(df)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920b337b",
   "metadata": {},
   "source": [
    "# Categorize (local, statewide, statewide trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1257bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 4 Robust Version ‚Äî Deterministic + LLM Comparison\n",
    "\n",
    "Creates:\n",
    "\n",
    "Local Rule (Strict)\n",
    "Local Rule (Expanded)\n",
    "Local Rule (LLM)\n",
    "\n",
    "Statewide Rule (Strict)\n",
    "Statewide Rule (Expanded)\n",
    "Statewide Rule (LLM)\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "LLM_MODEL = \"gpt-5\"\n",
    "MAX_WORKERS = 500\n",
    "\n",
    "BASE_DIR = Path.cwd().parent\n",
    "\n",
    "INPUT_FILE = BASE_DIR / \"files\" / \"order_extraction\" / \"extracted_rule_bodies.xlsx\"\n",
    "OUTPUT_FILE = BASE_DIR / \"files\" / \"order_extraction\" / \"extracted_rule_bodies_llm_regex.xlsx\"\n",
    "\n",
    "# ============================================================\n",
    "# LLM SCHEMA\n",
    "# ============================================================\n",
    "\n",
    "class LocalClassification(BaseModel):\n",
    "    is_local: int  # 1 or 0\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LLM PROMPT\n",
    "# ============================================================\n",
    "\n",
    "def build_prompt(text: str) -> str:\n",
    "    return f\"\"\"\n",
    "Classify this court rule as either:\n",
    "\n",
    "1 = Local Rule (applies to specific county, superior court, justice court, municipal court, etc.)\n",
    "0 = Statewide Rule (applies statewide across Arizona courts)\n",
    "\n",
    "Examples of Local:\n",
    "- Rules of Practice for the Maricopa County Superior Court\n",
    "- Local Rules of Civil Procedure, Mohave County Superior Court\n",
    "\n",
    "Examples of Statewide:\n",
    "- Arizona Rules of Civil Procedure\n",
    "- Arizona Rules of Criminal Procedure\n",
    "\n",
    "Return only:\n",
    "is_local: 0 or 1\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "\n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "    df[\"order_title\"] = df[\"order_title\"].astype(str)\n",
    "\n",
    "    # ============================================================\n",
    "    # STRICT DETERMINISTIC\n",
    "    # ============================================================\n",
    "\n",
    "    df[\"Local Rule (Strict)\"] = df[\"order_title\"].str.contains(\n",
    "        r\"\\blocal rules?\\b\",\n",
    "        case=False,\n",
    "        na=False\n",
    "    ).astype(int)\n",
    "\n",
    "    df[\"Statewide Rule (Strict)\"] = 1 - df[\"Local Rule (Strict)\"]\n",
    "\n",
    "    # ============================================================\n",
    "    # EXPANDED DETERMINISTIC\n",
    "    # ============================================================\n",
    "\n",
    "    county_pattern = r\"(Maricopa|Pima|Coconino|Yavapai|Mohave|Pinal|Yuma|Navajo|Gila|Cochise|Santa Cruz|La Paz|Greenlee|Graham|Apache)\"\n",
    "    trial_pattern = r\"(Superior Court|Justice Court|Municipal Court)\"\n",
    "\n",
    "    mentions_county = df[\"order_title\"].str.contains(\n",
    "        county_pattern,\n",
    "        case=False,\n",
    "        na=False\n",
    "    )\n",
    "\n",
    "    mentions_trial = df[\"order_title\"].str.contains(\n",
    "        trial_pattern,\n",
    "        case=False,\n",
    "        na=False\n",
    "    )\n",
    "\n",
    "    expanded_local = (\n",
    "        (df[\"Local Rule (Strict)\"] == 1) |\n",
    "        (mentions_county & mentions_trial)\n",
    "    )\n",
    "\n",
    "    df[\"Local Rule (Expanded)\"] = expanded_local.astype(int)\n",
    "    df[\"Statewide Rule (Expanded)\"] = 1 - df[\"Local Rule (Expanded)\"]\n",
    "\n",
    "    # ============================================================\n",
    "    # LLM CLASSIFICATION\n",
    "    # ============================================================\n",
    "\n",
    "    llm = ChatOpenAI(model=LLM_MODEL)\n",
    "    structured_llm = llm.with_structured_output(LocalClassification)\n",
    "\n",
    "    def classify(text):\n",
    "        try:\n",
    "            result = structured_llm.invoke(build_prompt(text))\n",
    "            return int(result.is_local)\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    llm_results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = [\n",
    "            executor.submit(classify, text)\n",
    "            for text in df[\"order_title\"]\n",
    "        ]\n",
    "\n",
    "        with tqdm(total=len(futures), desc=\"LLM Classification\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                llm_results.append(future.result())\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Keep original order\n",
    "    df[\"Local Rule (LLM)\"] = llm_results\n",
    "    df[\"Statewide Rule (LLM)\"] = 1 - df[\"Local Rule (LLM)\"]\n",
    "\n",
    "    # ============================================================\n",
    "    # SAVE\n",
    "    # ============================================================\n",
    "\n",
    "    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_excel(OUTPUT_FILE, index=False)\n",
    "\n",
    "    print(\"\\nFinished.\")\n",
    "    print(\"Saved to:\", OUTPUT_FILE)\n",
    "\n",
    "    # Quick comparison summary\n",
    "    print(\"\\nComparison:\")\n",
    "    print(\"Strict Local:\", df[\"Local Rule (Strict)\"].sum())\n",
    "    print(\"Expanded Local:\", df[\"Local Rule (Expanded)\"].sum())\n",
    "    print(\"LLM Local:\", df[\"Local Rule (LLM)\"].sum())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46856c72",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b4f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "BASE_DIR = Path.cwd().parent\n",
    "file_path = BASE_DIR / \"files\" / \"order_extraction\" / \"extracted_rule_bodies_llm_regex.xlsx\"\n",
    "\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Ensure issued_year exists\n",
    "if \"issued_year\" not in df.columns:\n",
    "    df[\"issued_year\"] = pd.to_datetime(\n",
    "        df[\"issued_date\"], errors=\"coerce\"\n",
    "    ).dt.year\n",
    "\n",
    "# Keep years 1961+\n",
    "df = df[df[\"issued_year\"] >= 1961]\n",
    "\n",
    "# ============================================================\n",
    "# FILTER: STATEWIDE (REGEX CLASSIFICATION)\n",
    "# ============================================================\n",
    "\n",
    "statewide = df[df[\"Local Rule (Expanded)\"] == 0]\n",
    "\n",
    "yearly_counts = (\n",
    "    statewide\n",
    "    .groupby(\"issued_year\")\n",
    "    .size()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# PLOT\n",
    "# ============================================================\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 12,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "ax.plot(\n",
    "    yearly_counts.index,\n",
    "    yearly_counts.values,\n",
    "    linewidth=2.5\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    \"Fig. 2. Statewide Rule Changes Issued Per Year\\nArizona Supreme Court\",\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Number of Statewide Rule Changes\")\n",
    "\n",
    "ax.set_xlim(1961, yearly_counts.index.max())\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77495c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "BASE_DIR = Path.cwd().parent\n",
    "file_path = BASE_DIR / \"files\" / \"order_extraction\" / \"extracted_rule_bodies_llm_regex.xlsx\"\n",
    "\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Ensure issued_year exists\n",
    "if \"issued_year\" not in df.columns:\n",
    "    df[\"issued_year\"] = pd.to_datetime(\n",
    "        df[\"issued_date\"], errors=\"coerce\"\n",
    "    ).dt.year\n",
    "\n",
    "df = df[df[\"issued_year\"] >= 1961]\n",
    "\n",
    "# ============================================================\n",
    "# RESTRICT TO TRIAL COURT RULES\n",
    "# ============================================================\n",
    "\n",
    "trial_mask = df[\"order_title\"].str.contains(\n",
    "    r\"Superior Court|Justice Court|Municipal Court\",\n",
    "    case=False,\n",
    "    na=False\n",
    ")\n",
    "\n",
    "trial_df = df[trial_mask].copy()\n",
    "\n",
    "# ============================================================\n",
    "# CREATE DECADE BINS\n",
    "# ============================================================\n",
    "\n",
    "def assign_decade(year):\n",
    "    if 1961 <= year <= 1970:\n",
    "        return \"1961-70\"\n",
    "    elif 1971 <= year <= 1980:\n",
    "        return \"1971-80\"\n",
    "    elif 1981 <= year <= 1990:\n",
    "        return \"1981-90\"\n",
    "    elif 1991 <= year <= 2000:\n",
    "        return \"1991-00\"\n",
    "    elif 2001 <= year <= 2010:\n",
    "        return \"2001-10\"\n",
    "    elif 2011 <= year <= 2020:\n",
    "        return \"2011-20\"\n",
    "    elif 2021 <= year <= 2024:\n",
    "        return \"2021-24\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "trial_df[\"decade\"] = trial_df[\"issued_year\"].apply(assign_decade)\n",
    "trial_df = trial_df.dropna(subset=[\"decade\"])\n",
    "\n",
    "# ============================================================\n",
    "# COUNT LOCAL VS STATEWIDE (REGEX)\n",
    "# ============================================================\n",
    "\n",
    "counts = (\n",
    "    trial_df\n",
    "    .groupby([\"decade\", \"Local Rule (Expanded)\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "counts = counts.rename(columns={0: \"Statewide\", 1: \"Local\"})\n",
    "\n",
    "# Ensure both columns exist\n",
    "if \"Local\" not in counts.columns:\n",
    "    counts[\"Local\"] = 0\n",
    "if \"Statewide\" not in counts.columns:\n",
    "    counts[\"Statewide\"] = 0\n",
    "\n",
    "counts = counts[[\"Local\", \"Statewide\"]]\n",
    "\n",
    "# Compute shares\n",
    "shares = counts.div(counts.sum(axis=1), axis=0)\n",
    "\n",
    "# ============================================================\n",
    "# PLOT STACKED BAR\n",
    "# ============================================================\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 12,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 6))\n",
    "\n",
    "bottom = np.zeros(len(shares))\n",
    "\n",
    "colors = {\n",
    "    \"Local\": \"#4D4D4D\",\n",
    "    \"Statewide\": \"#BFBFBF\"\n",
    "}\n",
    "\n",
    "for col in [\"Local\", \"Statewide\"]:\n",
    "    ax.bar(\n",
    "        shares.index,\n",
    "        shares[col] * 100,\n",
    "        bottom=bottom,\n",
    "        color=colors[col],\n",
    "        label=col\n",
    "    )\n",
    "    bottom += shares[col] * 100\n",
    "\n",
    "# ============================================================\n",
    "# ADD COUNTS INSIDE BARS\n",
    "# ============================================================\n",
    "\n",
    "for i, decade in enumerate(shares.index):\n",
    "    local_count = counts.loc[decade, \"Local\"]\n",
    "    statewide_count = counts.loc[decade, \"Statewide\"]\n",
    "\n",
    "    # Local count label\n",
    "    if local_count > 0:\n",
    "        ax.text(\n",
    "            i,\n",
    "            (shares.loc[decade, \"Local\"] * 100) / 2,\n",
    "            f\"{local_count}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"white\",\n",
    "            fontsize=11\n",
    "        )\n",
    "\n",
    "    # Statewide count label\n",
    "    if statewide_count > 0:\n",
    "        ax.text(\n",
    "            i,\n",
    "            shares.loc[decade, \"Local\"] * 100 +\n",
    "            (shares.loc[decade, \"Statewide\"] * 100) / 2,\n",
    "            f\"{statewide_count}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"black\",\n",
    "            fontsize=11\n",
    "        )\n",
    "\n",
    "# ============================================================\n",
    "# FORMATTING\n",
    "# ============================================================\n",
    "\n",
    "ax.set_ylabel(\"Percent of Trial Court Rule Changes\")\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "ax.set_title(\n",
    "    \"Fig. 3. Comparison of the Share of Centralized v. Local\\nRulemaking by Decade\",\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
